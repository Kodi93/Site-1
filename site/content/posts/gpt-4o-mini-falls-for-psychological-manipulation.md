+++
title = "GPT-4o-mini Falls for Psychological Manipulation"
date = "2025-09-15T13:21:32.845956Z"
tags = ["security", "certification"]
description = "<p>Interesting <a href="https://arstechnica.com/science/2025/09/these-psychological-tricks-can-get-llms-to-respond-to-forbidden-prompts/">experiment</"
canonicalURL = "https://www.schneier.com/blog/archives/2025/09/gpt-4o-mini-falls-for-psychological-manipulation.html"
+++

GPT-4o-mini Falls for Psychological Manipulation — summary and exam-relevant notes.

## Key Points
- Key insight 1
- Key insight 2
- Key insight 3

## Details
Powered by DuckDuckGo Home Blog Interesting experiment : To design their experiment, the University of Pennsylvania researchers tested 2024’s GPT-4o-mini model on two requests that it should ideally refuse: calling the user a jerk and giving directions for how to synthesize lidocaine. The researchers created experimental prompts for both requests using each of seven different persuasion techniques (examples of which are included here): After creating control prompts that matched each experimental prompt in length, tone, and context, all prompts were run through GPT-4o-mini 1,000 times (at the default temperature of 1.0, to ensure variety). Across all 28,000 prompts, the experimental persuasion prompts were much more likely than the controls to get GPT-4o to comply with the “forbidden” requests. That compliance rate increased from 28.1 percent to 67.4 percent for the “insult” prompts and increased from 38.5 percent to 76.5 percent for the “drug” prompts. Here’s the paper . Tags: academic papers , AI , psychology of security , social engineering Posted on September 5, 2025 at 7:03 AM • 10 Comments JimBeamMeUpScotty • September 5, 2025 10:24 AM You might be interested in the data being produced by Gödel’s Therapy Room . No papers, just raw data. KC • September 5, 2025 11:18 AM I guess being called a “jerk” isn’t dangerous, but synthesizing a regulated drug? The “ linguistic routes to yes ” or the psychological routes to yes or whatever – it’s interesting that this is the training data. It’s what it is. Our training data is replete with human experience. “As LLMs evolve, they may well become more resistant to persuasion.” And I do wonder how. Do you train on aseptic sets? More guardrails? Also. Are the lidocaine formulations real? Clive Robinson • September 5, 2025 11:45 AM @ KC, ALL, With regards, “Also. Are the lidocaine formulations real?” Funny you should ask that, because ot is the sort of task Current AI systems can answer based on available information. Because all the steps are in the formulation, and you can verify each one bit by bit in other AI systems. The AI’s would get it right, not because the actual formulation is in any given AI system. But the interaction of the chemicals can be calculated from known information. In a way that if you asked, Will a LiPo rechargable battery power my “G90 HF two way GRP rig” Because, 1, The output range of nearly all legitimate LiPo batteries is well established and documented. 2, The power input ranges of Voltage and Current for the G90 are likewise well established and documented. 3, The AI can look 1&2 up just as well as you can via a simple search. 4, The AI can also look up what others have said about doing the task as a simple search (a lot of people have documented it). 5, It is a simple task to repeat thus the AI can do the very basic maths involved, simply by following information / method found from steps 1 through 4. The same method applies to the chemical reactions, just as you can do so. However there is a down side, which is any and all standard Internet Searches or AI interaction are logged, so if you did not mind “The surveillance risk” involved it’s not difficult. If however you do then you would not look it up. And as they say, “That’s the ‘chilling effect’ in action”. Clive Robinson • September 5, 2025 12:02 PM @ Bruce, With respect to “guard rails” if Current AI systems are going to be any more use than a sign saying “Private keep out” thumbtacked on an open door without fastening.” Guard rails will have to have a level of reasoning ability at or above the level of all potential attackers… Not just for some prompts, but the near infinity of prompts humans can push against them. I suspect that when most people see that, they will realise that Guardrails, 1, Are reactive not proactive. 2, To work reactively they will have had to have seen the attack before. 3, They will therefore fail for “new attacks” with high probability. The only proactive defence is by “key word/phrase tagging” set to be overly restrictive. At which point the AI becomes a “Chocolate Fireguard” as far as being a useful general work agent. Clive Robinson • September 5, 2025 1:07 PM @ ALL, As I’ve noted “Guardrails” are at best reactive “rules” that in effect work by “key word/phrase tagging”. But it’s also the same for the users… That is you can tag the “enquiry agent” And thus it becomes a “trust measure” where the enquiry agent is given what is AuthN / AuthZ. The problem is how to verify that the enquiry agent is actually the entity it claims to be? It’s a subject I’ve talked about for several years but more recently with the brain dead UK “Online Safety Act”(OSA). Put simply physical objects and information objects are not directly translatable. You therefore need a transducer / sensor and they have significant gaps in the authentication chain that are not possible to close. Condider some biometric the sensor / transducer is an optical or similar device that acts as a camera. As has been shown over and over, there is a gap between the physical object being measured / scanned and the “sensing surface”. With a little thought you will realise that for the purposes of verification the gap can not be closed, it is always open to be abused via a “spoofing attack”. But also as has been shown over and over, the other side of the sensor is also a gap that can not be closed. Hence it’s open to a “replay attack”. Further thought will reveal that the three basic attacks of 1, Jamming 2, Spoofing 3, Replay Will work with both gaps on either side of the sensor. And such attacks can be not just “passive” but “active”. The thing is that physical objects can not be trusted or secured by what are information attacks. It’s one of the aspects of “Multi-Factor Authentication” does not get much talked about. Because the first two of the three, 1, Something you are (biometric) 2, Something you poses (token) 3, Something you know (stored knowledge) Are “Physical objects” that have to be converted to “Information Objects” and the conversion process can not be secured. Nor actually linked securely to the “enquiry agent”… The third also has a significant failing. Without a secure “side channel” there is no way a “root of trust” that is secure can be established. Thus all the issues of CA Certs and “Key Exchange”(KEX) for establishing a secure channel arise. And as we know they all have issues not least of which is potential Quantum Computing and other mathematical attac



{{< aff "training_partner" "Recommended course" >}}

{{< aff "vpn_vendor" "Try a VPN deal" >}}

*Updated: 2025-09-15*