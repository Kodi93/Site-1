+++
title = "The Risks of Code Assistant LLMs: Harmful Content, Misuse and Deception"
date = "2025-09-17T13:21:30.971181Z"
tags = ["security", "certification"]
description = "<p>We examine security weaknesses in LLM code assistants. Issues like indirect prompt injection and model misuse are prevalent across platforms.</p>
<"
canonicalURL = "https://unit42.paloaltonetworks.com/code-assistant-llms/"
+++

The Risks of Code Assistant LLMs: Harmful Content, Misuse and Deception — summary and exam-relevant notes.

## Key Points
- Key insight 1
- Key insight 2
- Key insight 3

## Details
We recently looked into AI code assistants that connect with integrated development environments (IDEs) as a plugin, much like GitHub Copilot. We found that both users and threat actors could misuse code assistant features like chat, auto-completion and writing unit tests for harmful purposes. This misuse includes injecting backdoors, leaking sensitive information and generating harmful content. We discovered that context attachment features can be vulnerable to indirect prompt injection . To set up this injection, threat actors first contaminate a public or third-party data source by inserting carefully crafted prompts into the source. When a user inadvertently supplies this contaminated data to an assistant, the malicious prompts hijack the assistant. This hijack could manipulate victims into executing a backdoor, inserting malicious code into an existing codebase and leaking sensitive information. In addition, users can manipulate assistants into generating harmful content by misusing auto-complete features, in a similar manner to recently identified content moderation bypasses on GitHub Copilot . Some AI assistants invoke their base model directly from the client. This exposes models to several additional risks, such as misuse by users or by external adversaries who are looking to sell access to LLM models . These weaknesses are likely to impact a number of LLM code assistants. Developers should implement standard security practices for LLMs, to ensure that environments are protected from the exploits discussed in this article. Conducting thorough code reviews and exercising control over LLM outputs will help secure AI-driven development against evolving threats. If you think you might have been compromised or have an urgent matter, contact the Unit 42 Incident Response team . Palo Alto Networks customers are better protected from the threats discussed above through the following products and services: While AI tool use in development processes keeps increasing, some of the risks associated with these tools — such as those used for code generation, refactoring and bug detection — can be overlooked. These risks include prompt injection and model misuse, which can lead to unintended behavior. The 2024 Stack Overflow Annual Developer Survey revealed that 76% of all respondents are using or plan to use AI tools in their development process. Among developers currently using AI tools, 82% reported using them to write code. The rapid adoption of AI tools, particularly large language models (LLMs), has significantly transformed the way developers approach coding tasks. LLM-based coding assistants have become integral parts of modern development workflows. These tools leverage natural language processing to understand developer intent, generate code snippets and provide real-time suggestions, potentially reducing the time and effort spent on manual coding. Some of these tools have gained attention for their deep integration with existing codebases and their ability to assist developers in navigating complex projects. AI-driven coding assistants are also prone to potential security concerns that could impact development processes. The weaknesses we identified are likely to be present in a variety of IDEs, versions, models and even different products that use LLMs as coding assistants. The core of prompt injection vulnerabilities lies in a model’s inability to reliably distinguish between system instructions (code) and user prompts (data). This mixture of data and code has always been an issue in computing, leading to vulnerabilities such as SQL injections, buffer overflows and command injections. LLMs face a similar risk because they process both instructions and user inputs in the same way. This behavior makes them susceptible to prompt injection , where adversaries craft inputs that manipulate LLMs into unintended behavior. System prompts are instructions that guide the AI’s behavior, defining its role and ethical boundaries for the application. User inputs are the dynamic questions, commands or even external data (like documents or web content) that a person supplies to the LLM-based application. Because the LLM receives all types of input as natural language text, attackers can craft malicious user inputs that mimic or override system prompts, bypassing safeguards and influencing the LLM’s responses. ​​This indistinguishable nature of instructions and data also gives rise to indirect prompt injection , which presents an even greater challenge. Instead of directly injecting malicious input, adversaries embed harmful prompts within these external data sources, such as websites, documents or APIs that the LLM processes. Once the LLM processes this compromised external data (either directly or when a user unknowingly submits it), it will follow the instructions specified in the embedded malicious prompt. This allows it to bypass traditional safeguards and cause unintended behavior. Figure 1 illustrates the difference between direct and indirect prompt injections. Traditional LLMs often operate with a knowledge cutoff, meaning their training data doesn’t include the most current information or highly specific details relevant to a user’s local codebase or proprietary systems. This creates a significant knowledge gap when developers need assistance with their specific projects. To overcome this limitation and enable more precise, context-aware responses, LLM tools implement features that allow users to explicitly provide external context, bridging this gap by feeding the relevant data directly into the LLM. One feature offered by some coding assistants is the ability to attach context in the form of a specific file, folder, repository or URL. Adding this context to prompts enables the code assistant to provide more accurate and specific output. However, this feature could also create an opportunity for indirect prompt injection attacks if users unintentionally provide context sources that threat actors have contaminated. Behind the scenes, when a user adds context to an instruction, the model processes this context as a prompt that precedes the user’s actual prompt. Figure 2 shows this chat structure. Since this content can come from external sources, such as a URL or a file outside the current repository, users risk unknowingly attaching malicious context that could contain indirect prompt injections. As a prominent social media platform, X (formerly known as Twitter) is a vast and fr



{{< aff "training_partner" "Recommended course" >}}

{{< aff "vpn_vendor" "Try a VPN deal" >}}

*Updated: 2025-09-17*